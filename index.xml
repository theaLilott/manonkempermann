<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Manon Kempermann</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on Manon Kempermann</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-gb</language>
    <lastBuildDate>Tue, 08 Apr 2025 15:00:29 +0200</lastBuildDate><atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>The Challenge of Measuring Safety in LLMs</title>
      <link>http://localhost:1313/blog/challenge_of_evals/challenge_of_measuring_safety/</link>
      <pubDate>Tue, 08 Apr 2025 15:00:29 +0200</pubDate>
      
      <guid>http://localhost:1313/blog/challenge_of_evals/challenge_of_measuring_safety/</guid>
      
      <description>&lt;p&gt;Before I turn to the challenge of measuring safety of LLMs, I want to give a short clarification on why I think we need to care about the safety of LLMs or AI systems in general.&lt;/p&gt;
&lt;p&gt;In contrast to other technology humankind has invented before, AI is probably the first one that brings us immense use without us understanding how it works. AI systems are inherently complex and unpredictable systems. They live as a product in the complex economic market and are ultimately used in the complex system of society. An error on any level of this chain can lead to unforeseeable and potentially catastrophic outcome as the effect ripples through to the other levels. Therefore, any individual risk factor of AI is likely to lead to a chain of other risks.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Scalable Oversight via Debate</title>
      <link>http://localhost:1313/blog/scalable-oversight/scalable_oversight/</link>
      <pubDate>Sun, 02 Mar 2025 20:23:06 +0100</pubDate>
      
      <guid>http://localhost:1313/blog/scalable-oversight/scalable_oversight/</guid>
      
      <description>&lt;hr&gt;
&lt;p&gt;This post, I wrote for and published on the &lt;a href=&#34;https://updates.i2sc.net/2025/01/blog-scalable-oversight-via-debate.html&#34;&gt;I2SC blog&lt;/a&gt; after my talk to the group about the same topic in our weekly meetings. I presented on a recent paper, that I came across for my current research project, but that also fits into the scope of AI safety. This post is therefore a bit different from my other blog posts in that I look at a specific piece of research instead of discussing a wider topic.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Dangerous AI - An Overwiew of AI Risks</title>
      <link>http://localhost:1313/blog/25_02_ai_risks_overview/</link>
      <pubDate>Sat, 01 Mar 2025 19:11:27 +0100</pubDate>
      
      <guid>http://localhost:1313/blog/25_02_ai_risks_overview/</guid>
      
      <description>&lt;p&gt;I am currently taking part in the &lt;em&gt;AI Safety, Ethics, and Society&lt;/em&gt; course by the Center for AI Safety (CAIS)&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; and will be writing about some key learnings from the course and related discussions here. This week, we explored the various risks associated with AI. While I have been reading extensively about AI safety—and, inevitably, AI risks—this discussion brought my attention to certain risks I hadn&amp;rsquo;t previously considered. At the same time, as I engage more in conversations about AI with friends from different disciplines, I increasingly realize how little awareness exists in the public about these risks.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Alignment or why we need to think about AI safety</title>
      <link>http://localhost:1313/blog/25_01_intro_alignment_problem/</link>
      <pubDate>Thu, 02 Jan 2025 10:23:21 +0100</pubDate>
      
      <guid>http://localhost:1313/blog/25_01_intro_alignment_problem/</guid>
      
      <description>&lt;p&gt;I want to begin this blog with an outline of the problem with AI and why we need to pay attention to AI safety.&lt;/p&gt;
&lt;p&gt;Recently, I caught up with a friend who countered my interest in AI safety by claiming that this focus was “so German”—always overly cautious, regulating everything too much, and thereby slowing down innovation and blocking AI’s potential. While his argument has some merit, I believe there’s a deeper dimension to how AI could go wrong—one that goes beyond misuse, which was essentially his concern. This dimension is referred to as the alignment problem, and it’s what we’ll explore today.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Why I write this blog</title>
      <link>http://localhost:1313/blog/24_52_kick_of_blog/</link>
      <pubDate>Tue, 31 Dec 2024 10:22:41 +0100</pubDate>
      
      <guid>http://localhost:1313/blog/24_52_kick_of_blog/</guid>
      
      <description>&lt;p&gt;When I started studying Data Science and AI, I was almost entirely interested in the application of AI in other domains of science such as biology or environmental science. I had never really programmed before and did not see myself as a computer scientist. However, having some knowledge about AI seemed promising in these times and liking STEM subjects more than social science, I decided to give it a try.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>CV</title>
      <link>http://localhost:1313/page/cv/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/page/cv/</guid>
      
      <description>&lt;h1 id=&#34;manon-kempermann&#34;&gt;Manon Kempermann&lt;/h1&gt;
&lt;h3 id=&#34;education&#34;&gt;Education&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;BSc &lt;strong&gt;Data Science and AI&lt;/strong&gt;, Saarland University, Germany (2023 - 2026)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Average grade: 1.2 (~3.8 GPA)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Relevant Class work:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;MIPS assembly&lt;/strong&gt;: Rock, Paper, Scissors including random number simulator&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;C&lt;/strong&gt;: Image edge detector, PageRank&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Java&lt;/strong&gt;: 2048 Game, Route planner, Compiler for C subset to MIPS assembly&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Scottish Qualification Certificate, Kilgraston School, Scotland, UK (2021 - 2022)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dresden Trust Scholar&lt;/li&gt;
&lt;li&gt;Highers in Mathematics (A), Chemistry (A), Human Biology (A), Business Management (A), Latin (A), German (A)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Young Student, Technical University Dresden, Germany (2019 - 2021)&lt;/p&gt;</description>
      
    </item>
    
  </channel>
</rss>
